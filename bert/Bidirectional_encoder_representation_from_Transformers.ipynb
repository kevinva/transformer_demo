{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSz5jzj61nHc"
   },
   "source": [
    "This code is possible because of [Tae-Hwan Jung](https://github.com/graykode). I have just broken down the code and added few things here and here for better understanding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-8kZmr4ItGUj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w6YMNvc8tbA9"
   },
   "outputs": [],
   "source": [
    "text = (\n",
    "        'Hello, how are you? I am Romeo.\\n'\n",
    "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "        'Nice meet you too. How are you today?\\n'\n",
    "        'Great. My baseball team won the competition.\\n'\n",
    "        'Oh Congratulations, Juliet\\n'\n",
    "        'Thanks you Romeo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AhX8b1ydtrVf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list:  ['baseball', 'thanks', 'romeo', 'oh', 'name', 'you', 'is', 'my', 'team', 'nice', 'i', 'hello', 'competition', 'too', 'meet', 'great', 'how', 'congratulations', 'to', 'are', 'today', 'the', 'juliet', 'am', 'won']\n"
     ]
    }
   ],
   "source": [
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "print('word_list: ', word_list)\n",
    "\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello how are you i am romeo',\n",
       " 'hello romeo my name is juliet nice to meet you',\n",
       " 'nice meet you too how are you today',\n",
       " 'great my baseball team won the competition',\n",
       " 'oh congratulations juliet',\n",
       " 'thanks you romeo']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ42SFLKtsv_",
    "outputId": "16c28ac8-8349-48ab-f1d3-a9431e658349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15, 20, 23, 9, 14, 27, 6],\n",
       " [15, 6, 11, 8, 10, 26, 13, 22, 18, 9],\n",
       " [13, 18, 9, 17, 20, 23, 9, 24],\n",
       " [19, 11, 4, 12, 28, 25, 16],\n",
       " [7, 21, 26],\n",
       " [5, 9, 6]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q03SGkfIu_Kd"
   },
   "outputs": [],
   "source": [
    "maxlen = 30 # maximum of length\n",
    "batch_size = 6\n",
    "max_pred = 5  # max tokens of prediction\n",
    "n_layers = 6 # number of Encoder of Encoder Layer\n",
    "n_heads = 12 # number of heads in Multi-Head Attention\n",
    "d_model = 768 # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TtyOOmRntu8w"
   },
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
    "#         print('tokens_a_index:', tokens_a_index)\n",
    "#         print('tokens_b_index:', tokens_b_index)\n",
    "        \n",
    "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "#         print('tokens_a:', tokens_a)\n",
    "        # 选出随机两个句子\n",
    "\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        print('inputs_ids:', input_ids)\n",
    "        # 按照bert的约定，构造输入数据\n",
    "\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        # 前后两个句子的编码\n",
    "\n",
    "        #MASK LM\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "        # 因为要随机mask掉15%的单词，计算句子15%的长度\n",
    "        \n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        # 取出可以mask的索引位置\n",
    "        print('cand_maked_pos: ', cand_maked_pos)\n",
    "        \n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        # masked_pos选出要mask的索引\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80%  # 要mask的token有80%的几率替换为[MASK]标记\n",
    "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "            elif random() < 0.5:  # 10%  # 剩下有%10的几率被替换为其他token，另外有10%几率原封不动\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)  # 看来对句子嵌入的padding不用考虑现有的是0或1，直接用0填充！！！\n",
    "        print('1 masked_tokens: ', masked_tokens)\n",
    "        print('1 masked_pos: ', masked_pos)\n",
    "        \n",
    "    #     # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "        print('2 masked_tokens: ', masked_tokens)\n",
    "        print('2 masked_pos: ', masked_pos)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext(tokens_b是tokens_a的下一个句子)\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext(tokens_b不是tokens_a的下一个句子)\n",
    "            negative += 1\n",
    "    return batch\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "s1PGksqBNuZM"
   },
   "outputs": [],
   "source": [
    "# 获取mask矩阵\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # 入参seq_q、seq_k就是句子的tokens的batch\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "#     print('before expand: ', pad_attn_mask)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lgJwW4OaiXE2"
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Q7_HC-Y0jC3K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18]\n",
      "1 masked_tokens:  [13, 26, 8]\n",
      "1 masked_pos:  [7, 6, 4]\n",
      "2 masked_tokens:  [13, 26, 8, 0, 0]\n",
      "2 masked_pos:  [7, 6, 4, 0, 0]\n",
      "inputs_ids: [1, 5, 9, 6, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11]\n",
      "1 masked_tokens:  [12, 16]\n",
      "1 masked_pos:  [8, 11]\n",
      "2 masked_tokens:  [12, 16, 0, 0, 0]\n",
      "2 masked_pos:  [8, 11, 0, 0, 0]\n",
      "inputs_ids: [1, 15, 20, 23, 9, 14, 27, 6, 2, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "1 masked_tokens:  [23, 8, 15]\n",
      "1 masked_pos:  [3, 12, 9]\n",
      "2 masked_tokens:  [23, 8, 15, 0, 0]\n",
      "2 masked_pos:  [3, 12, 9, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 7, 21, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14]\n",
      "1 masked_tokens:  [10, 11]\n",
      "1 masked_pos:  [5, 3]\n",
      "2 masked_tokens:  [10, 11, 0, 0, 0]\n",
      "2 masked_pos:  [5, 3, 0, 0, 0]\n",
      "inputs_ids: [1, 13, 18, 9, 17, 20, 23, 9, 24, 2, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "1 masked_tokens:  [22, 10, 26]\n",
      "1 masked_pos:  [17, 14, 15]\n",
      "2 masked_tokens:  [22, 10, 26, 0, 0]\n",
      "2 masked_pos:  [17, 14, 15, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18]\n",
      "1 masked_tokens:  [10, 26, 11]\n",
      "1 masked_pos:  [5, 6, 3]\n",
      "2 masked_tokens:  [10, 26, 11, 0, 0]\n",
      "2 masked_pos:  [5, 6, 3, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 13, 18, 9, 17, 20, 23, 9, 24, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "1 masked_tokens:  [9, 20, 11]\n",
      "1 masked_pos:  [10, 16, 3]\n",
      "2 masked_tokens:  [9, 20, 11, 0, 0]\n",
      "2 masked_pos:  [10, 16, 3, 0, 0]\n",
      "inputs_ids: [1, 19, 11, 4, 12, 28, 25, 16, 2, 13, 18, 9, 17, 20, 23, 9, 24, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "1 masked_tokens:  [25, 17, 18]\n",
      "1 masked_pos:  [6, 12, 10]\n",
      "2 masked_tokens:  [25, 17, 18, 0, 0]\n",
      "2 masked_pos:  [6, 12, 10, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18]\n",
      "1 masked_tokens:  [16, 9, 11]\n",
      "1 masked_pos:  [18, 10, 13]\n",
      "2 masked_tokens:  [16, 9, 11, 0, 0]\n",
      "2 masked_pos:  [18, 10, 13, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "1 masked_tokens:  [9, 26, 15]\n",
      "1 masked_pos:  [21, 17, 1]\n",
      "2 masked_tokens:  [9, 26, 15, 0, 0]\n",
      "2 masked_pos:  [21, 17, 1, 0, 0]\n",
      "inputs_ids: [1, 15, 20, 23, 9, 14, 27, 6, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]\n",
      "1 masked_tokens:  [15, 6, 25]\n",
      "1 masked_pos:  [1, 7, 14]\n",
      "2 masked_tokens:  [15, 6, 25, 0, 0]\n",
      "2 masked_pos:  [1, 7, 14, 0, 0]\n",
      "inputs_ids: [1, 15, 20, 23, 9, 14, 27, 6, 2, 5, 9, 6, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]\n",
      "1 masked_tokens:  [14, 27]\n",
      "1 masked_pos:  [5, 6]\n",
      "2 masked_tokens:  [14, 27, 0, 0, 0]\n",
      "2 masked_pos:  [5, 6, 0, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 7, 21, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14]\n",
      "1 masked_tokens:  [13, 21]\n",
      "1 masked_pos:  [7, 13]\n",
      "2 masked_tokens:  [13, 21, 0, 0, 0]\n",
      "2 masked_pos:  [7, 13, 0, 0, 0]\n",
      "inputs_ids: [1, 15, 20, 23, 9, 14, 27, 6, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]\n",
      "1 masked_tokens:  [11, 27, 12]\n",
      "1 masked_pos:  [10, 6, 12]\n",
      "2 masked_tokens:  [11, 27, 12, 0, 0]\n",
      "2 masked_pos:  [10, 6, 12, 0, 0]\n",
      "inputs_ids: [1, 15, 20, 23, 9, 14, 27, 6, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]\n",
      "1 masked_tokens:  [23, 20, 27]\n",
      "1 masked_pos:  [3, 2, 6]\n",
      "2 masked_tokens:  [23, 20, 27, 0, 0]\n",
      "2 masked_pos:  [3, 2, 6, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 13, 18, 9, 17, 20, 23, 9, 24, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "1 masked_tokens:  [22, 10, 18]\n",
      "1 masked_pos:  [8, 5, 9]\n",
      "2 masked_tokens:  [22, 10, 18, 0, 0]\n",
      "2 masked_pos:  [8, 5, 9, 0, 0]\n"
     ]
    }
   ],
   "source": [
    " batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XM1-FdPJi6p3"
   },
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "# zip(*batch)将每个batch对应的向量匹配起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 15,  6, 11,  3, 10,  3, 13, 22, 18,  9,  2, 19, 11,  4, 12, 28, 25,\n",
       "         16,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  5,  9,  6,  2, 19, 11,  4,  3, 28, 25,  3,  2,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 15, 20,  3,  9, 14, 27,  6,  2,  3,  6, 11,  3, 10, 26, 13, 22, 18,\n",
       "          9,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 15,  6,  3,  8,  3, 26, 13, 22, 18,  9,  2,  7, 21, 26,  2,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 15,  6,  3,  8, 10, 26, 13, 22, 18,  3,  2, 13, 18,  9, 17,  3, 23,\n",
       "          9, 24,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 15,  6, 11,  8,  3, 26, 13,  3,  3,  9,  2, 13, 18,  9, 17, 20, 23,\n",
       "          9, 24,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhM1DCU_iYCB",
    "outputId": "7525fdc7-f78e-488b-ef35-7d9ecfb969e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]),\n",
       " tensor([ 1, 15,  6, 11,  3, 10,  3, 13, 22, 18,  9,  2, 19, 11,  4, 12, 28, 25,\n",
       "         16,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attn_pad_mask(input_ids, input_ids)[0][0], input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Qnay0LTDjE4S"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "#         print('pos: ', pos)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEmbed = Embedding()\n",
    "testEmbed(input_ids[0].unsqueeze(0), segment_ids[0]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "rHjj-1wXjsdI"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        print('[ScaledDotProductAttention]Q: ', Q.size())\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "#         print('scores: ', scores)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "#         print('attn: ', attn)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return scores, context, attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m:  tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.9507e-02, -1.0000e+09],\n",
       "        [-5.4184e-01, -1.0000e+09],\n",
       "        [-1.0000e+09, -6.6837e-01]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(3, 2)\n",
    "m = torch.randint(0, 2, (3, 2))\n",
    "print('m: ', m)\n",
    "t.masked_fill_(m == 0, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X2rbGNMzl7o",
    "outputId": "81b17de0-c3ef-448e-896f-7143b39cbd62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attenM:  torch.Size([6, 30, 30])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 30, 768])\n",
      "S:  torch.Size([6, 30, 30])\n",
      "C:  torch.Size([6, 30, 768])\n",
      "A:  torch.Size([6, 30, 30])\n",
      "Context:  tensor([-1.2351e+00,  5.1993e-02, -2.0187e+00,  1.9864e+00,  2.9582e-01,\n",
      "        -3.3060e-01,  1.8451e+00,  1.4233e-01, -5.3186e-03,  1.1268e+00,\n",
      "        -2.3374e-01,  6.9273e-01, -1.3392e-01, -1.3834e+00,  1.2138e+00,\n",
      "         6.8754e-01,  1.7689e-01,  1.1074e+00, -3.2812e-01,  6.8222e-01,\n",
      "         6.1238e-02,  1.4365e+00, -2.7149e-01, -8.0325e-01, -2.6319e-02,\n",
      "        -1.2014e+00,  8.1103e-01, -1.0333e+00,  3.0978e-02,  9.8884e-01,\n",
      "        -7.8721e-01, -1.2162e+00, -1.2874e+00, -8.4791e-01,  4.1130e-01,\n",
      "         5.1632e-01, -4.8832e-01,  7.2632e-01,  7.8077e-02, -3.3885e-01,\n",
      "         1.3955e-01, -1.0874e+00, -3.0984e-01, -9.8336e-01,  5.2328e-01,\n",
      "        -6.2398e-01,  2.6939e-02,  6.7558e-01,  2.1289e-01,  7.5228e-01,\n",
      "         3.0185e+00,  1.1712e+00,  2.6628e-01,  1.3806e+00, -7.5192e-01,\n",
      "        -5.9001e-01, -5.1810e-01, -7.0553e-01, -1.5541e+00, -4.6116e-01,\n",
      "         1.8501e+00, -7.6210e-01, -4.9938e-01, -1.2709e-01, -2.3376e-01,\n",
      "        -3.2352e-01,  1.1103e-01, -6.4161e-03, -1.7356e+00,  2.3449e+00,\n",
      "        -1.7421e+00, -1.5380e+00,  1.1338e+00,  2.8950e-01, -2.1485e-01,\n",
      "        -2.8345e-01, -3.7326e-01, -5.3401e-01, -2.1045e-01, -1.6362e-02,\n",
      "         1.0078e-01, -1.5747e+00,  1.5500e+00,  7.1105e-01, -5.5358e-02,\n",
      "         6.7312e-01, -4.5457e-01, -5.3015e-01, -6.9098e-01,  5.1409e-01,\n",
      "        -2.7194e+00,  8.1301e-01,  7.0084e-01, -1.9505e+00, -6.3714e-01,\n",
      "        -1.2834e+00,  1.0244e+00,  7.2832e-01, -7.4056e-01,  1.1627e+00,\n",
      "         7.3058e-01,  1.0463e-01,  7.4975e-01, -5.7354e-01, -1.4682e+00,\n",
      "         1.8756e+00,  3.1514e-01,  1.2157e+00,  7.0412e-01,  7.4675e-01,\n",
      "         7.5450e-01,  4.7154e-01,  1.0314e+00, -1.0335e+00,  3.7768e-01,\n",
      "        -4.6897e-01, -4.4607e-01, -3.0527e-01, -9.2785e-01,  8.5822e-01,\n",
      "        -1.9837e-01,  4.6894e-01,  7.0385e-01,  1.7592e-01,  2.0615e-01,\n",
      "         7.0948e-02, -1.0085e+00,  9.9179e-01,  3.0217e-01,  5.1363e-01,\n",
      "        -7.5861e-01, -6.0087e-01,  1.1699e+00, -6.3220e-01,  8.1965e-01,\n",
      "         9.1976e-01, -2.1823e+00,  1.9533e+00,  7.9545e-01,  3.2977e-01,\n",
      "         2.6375e+00, -2.2233e-01, -6.8783e-02,  3.9027e-03,  2.4222e-01,\n",
      "        -4.5800e-01, -1.2644e+00, -2.6109e-01, -1.3839e+00, -1.0121e+00,\n",
      "         8.3552e-01,  2.6158e-01, -3.1644e-01, -3.6309e-01, -8.5938e-01,\n",
      "         7.5230e-01,  1.0148e+00,  1.3412e+00,  1.0002e+00,  9.7538e-01,\n",
      "        -3.0313e-01,  2.8010e+00,  8.0003e-01, -5.2976e-01,  1.0894e+00,\n",
      "         7.3278e-01,  1.1575e+00, -1.7895e+00,  3.8274e-01,  1.5249e+00,\n",
      "         1.2956e+00,  2.1191e+00, -1.5159e+00, -6.2646e-01, -8.9524e-01,\n",
      "         6.2137e-01, -1.5308e-01,  1.9733e-01,  6.3011e-01, -7.2039e-01,\n",
      "        -8.8911e-01,  1.5340e+00, -4.5923e-03, -2.1029e+00,  1.1239e+00,\n",
      "        -4.0956e-01,  1.7478e+00, -5.1399e-01, -2.8897e-02, -7.3276e-01,\n",
      "        -1.3152e+00, -4.3306e-01,  1.3024e-01,  6.9302e-01,  3.7644e-01,\n",
      "         2.3759e-01,  3.8142e-01,  8.4135e-01, -1.2323e+00, -5.1791e-01,\n",
      "        -8.8389e-01, -1.7133e+00,  4.2468e-01,  1.1805e+00,  6.5096e-03,\n",
      "        -1.9861e-01, -1.9601e+00,  1.4456e+00,  5.9990e-01,  7.0718e-01,\n",
      "        -3.3427e-01,  1.5439e-01, -1.7387e-01, -1.0093e+00, -2.3823e+00,\n",
      "        -1.6924e+00, -3.6147e-01, -4.8505e-02,  4.1238e-01, -3.0398e-01,\n",
      "         2.2070e+00, -8.7846e-02, -1.0059e+00, -1.9159e+00,  1.4374e+00,\n",
      "         5.6445e-01,  6.5173e-01,  2.0475e-01,  2.1007e+00, -2.2789e+00,\n",
      "         7.8062e-02, -4.8168e-01, -7.4905e-01,  6.6017e-02,  2.2132e+00,\n",
      "         3.6253e-01, -1.2450e+00,  1.4186e+00, -3.2048e-01,  2.0648e+00,\n",
      "         1.5867e-01, -9.1282e-03, -4.9531e-01, -9.0707e-02, -7.9911e-01,\n",
      "        -4.5068e-01,  2.8217e-02, -1.4243e+00, -5.8700e-01,  2.8759e-01,\n",
      "         1.2787e-01, -1.3164e+00, -6.6297e-01, -1.0839e+00,  4.3052e-01,\n",
      "        -2.4673e-01, -1.3907e+00,  1.9848e+00,  1.3241e+00,  2.0872e-01,\n",
      "         2.9663e-01,  1.9161e+00, -1.5799e-01, -8.7323e-01, -5.6163e-02,\n",
      "        -6.2909e-02,  7.4084e-01, -1.1439e+00,  2.6288e-01,  1.0429e+00,\n",
      "        -3.1739e-01,  2.0175e-01, -1.2547e+00,  1.9604e+00, -1.7288e+00,\n",
      "         1.3236e+00,  1.3406e+00, -5.0109e-01,  2.7172e-01, -3.3107e-01,\n",
      "        -5.6756e-01,  8.2051e-01,  1.9108e-01, -3.5505e-01,  1.7485e+00,\n",
      "        -3.7666e-01,  1.6283e+00, -8.0617e-01,  1.5797e+00, -1.2194e+00,\n",
      "        -1.0878e-01, -2.0538e-01, -1.7049e+00,  2.7379e-01,  7.1541e-01,\n",
      "         1.5086e+00, -1.9586e-01,  7.4765e-01,  4.8380e-01,  1.4222e+00,\n",
      "        -2.0675e+00,  1.2248e+00, -1.1450e+00, -3.3990e-01,  2.1729e+00,\n",
      "         5.3505e-01,  3.7996e-01, -1.6241e-01, -5.9720e-01,  3.2499e-02,\n",
      "        -4.6388e-01,  6.2753e-01, -1.5455e+00, -5.2816e-02,  2.5165e-01,\n",
      "        -1.1258e+00,  2.9875e-01, -9.4371e-01, -1.9926e+00,  5.0591e-01,\n",
      "        -5.2685e-01,  2.8946e-01,  1.2721e-01,  1.0481e+00, -1.6884e+00,\n",
      "        -1.0538e+00, -1.0028e+00, -4.7155e-01,  1.8941e-01,  7.0491e-01,\n",
      "        -4.9255e-01, -1.8764e-01, -1.3987e+00, -1.1060e+00,  7.6083e-01,\n",
      "        -2.1784e+00, -1.4873e+00, -8.6000e-01, -4.2372e-01,  1.2904e+00,\n",
      "         1.1260e+00,  2.4276e-01,  2.4221e-01, -2.6043e-01,  2.2369e+00,\n",
      "         1.4982e+00, -1.5859e-01, -1.6184e+00,  7.5537e-01,  1.1770e+00,\n",
      "         5.8476e-01,  7.0738e-01, -1.5518e+00,  3.1034e+00,  8.7308e-01,\n",
      "         1.4235e-02, -4.4838e-02, -1.5452e-01, -4.0421e-01,  1.9463e-01,\n",
      "         8.4827e-01, -1.3409e+00, -7.0302e-01, -1.0769e-01,  4.1700e-01,\n",
      "         1.6373e+00, -5.6507e-01, -3.7181e-01,  9.8838e-01,  7.7903e-01,\n",
      "        -7.5593e-01, -6.6038e-02,  5.1054e-01,  5.1826e-01, -5.7960e-01,\n",
      "         6.4811e-01,  4.5233e-01, -1.5578e-01,  1.4313e+00,  7.9683e-02,\n",
      "        -1.4864e-01,  1.4112e+00,  3.1474e-01,  2.0215e-01, -2.1183e+00,\n",
      "        -9.2770e-01, -6.5960e-01,  1.4410e-01,  2.0201e+00, -5.6988e-02,\n",
      "        -7.9786e-01, -1.3403e+00,  1.0704e+00, -2.8389e-03,  9.5800e-01,\n",
      "         1.0940e+00, -5.5592e-01, -4.3331e-01,  1.8033e+00, -1.0700e-01,\n",
      "        -7.0312e-01,  1.3821e+00,  9.7892e-01, -5.4037e-01, -1.7815e-01,\n",
      "        -1.5492e-01, -1.1382e-01, -1.8216e+00, -1.3648e+00, -2.1986e-01,\n",
      "        -7.6131e-01,  6.6976e-02,  2.3494e-01, -1.5077e-01, -1.8607e+00,\n",
      "        -6.8391e-01,  1.0943e+00, -5.4022e-01, -2.9661e-02,  1.3516e+00,\n",
      "         1.4259e+00,  4.3648e-01, -6.4251e-01, -2.7188e-01, -1.2734e+00,\n",
      "         8.0893e-01, -2.5211e-02, -8.6082e-01, -2.7657e-02, -4.2105e-01,\n",
      "         9.0556e-01, -8.1550e-01, -3.0770e-01,  6.3410e-01,  2.2796e-01,\n",
      "        -1.6750e+00, -4.6954e-01,  1.0044e+00, -1.6298e-01,  8.7475e-01,\n",
      "        -3.1059e-01, -4.4197e-01,  2.0891e+00, -2.2124e+00,  1.1687e+00,\n",
      "        -1.1607e+00,  3.6360e-02,  2.0627e-01, -1.9071e-01,  5.0529e-01,\n",
      "        -1.6458e+00, -7.0226e-01, -7.3248e-01, -1.6366e+00,  1.4844e+00,\n",
      "        -3.4422e-01,  2.1826e-01,  8.4719e-01, -2.9758e-01, -2.2896e-01,\n",
      "        -2.5366e-01, -8.8877e-01,  8.4843e-01,  1.4390e+00, -1.7549e+00,\n",
      "         2.9944e-02,  1.9791e+00,  3.8799e-01,  1.8444e+00, -6.2335e-02,\n",
      "        -1.7703e-01, -1.1784e+00, -2.0128e+00, -1.1084e-01,  6.7178e-01,\n",
      "        -3.3113e-01,  1.1433e-01, -3.2618e-01,  6.1125e-02, -3.7286e-01,\n",
      "         1.4869e+00,  1.9668e-01,  2.1830e-01,  2.9971e-01, -1.2629e-01,\n",
      "        -5.4348e-01, -8.3366e-01, -4.1758e-01,  1.9158e+00, -7.8496e-01,\n",
      "         4.5329e-01,  1.5063e+00,  9.5106e-01, -4.5354e-02, -7.8001e-01,\n",
      "         6.0242e-03, -1.4441e+00,  9.1560e-03,  8.4645e-02,  1.5582e-01,\n",
      "         8.6761e-01,  2.1117e-01,  8.1628e-01, -1.0146e+00, -1.4100e+00,\n",
      "         5.8399e-01, -3.8260e-01, -3.0878e-01,  9.4379e-01, -1.0815e+00,\n",
      "         4.7578e-01, -3.3614e-01,  5.7839e-01, -1.2574e+00, -9.9082e-01,\n",
      "        -2.4564e+00,  6.3110e-01, -1.0409e+00,  2.9574e-01, -1.0790e+00,\n",
      "         8.7776e-02,  3.3716e-02, -2.5280e-01,  2.1137e-01, -7.0875e-01,\n",
      "        -2.7705e-01,  6.5759e-02, -7.3660e-01, -3.9432e-01,  5.5164e-01,\n",
      "         6.6288e-01, -6.8680e-01,  3.8982e-01, -1.0969e+00, -6.0325e-01,\n",
      "        -6.6515e-01,  8.5625e-01, -8.6497e-01,  2.4260e-01,  2.7107e-01,\n",
      "        -6.5185e-01,  6.7711e-01,  2.8573e-01,  1.3577e+00,  1.3385e+00,\n",
      "        -1.8330e+00, -1.2135e-01,  2.4829e-01, -7.9448e-01,  4.9416e-02,\n",
      "         3.2873e-03,  8.2654e-02, -9.1697e-01, -8.2749e-01, -2.4307e+00,\n",
      "        -6.4148e-01,  9.7175e-01, -4.3783e-02,  3.6251e-01,  5.1869e-01,\n",
      "         5.6572e-01, -2.3852e+00,  6.0744e-01, -4.6513e-01,  8.6949e-02,\n",
      "        -2.6902e-02, -1.3867e+00,  1.5109e+00,  5.3811e-01, -6.5925e-01,\n",
      "         1.5742e+00,  3.0087e-01,  2.7731e-01, -3.5171e-01, -1.3891e+00,\n",
      "        -1.1100e+00,  4.3454e-01, -1.4122e+00,  1.6151e-02, -5.2352e-01,\n",
      "        -9.5255e-01,  8.8799e-01, -1.8100e+00, -1.8025e+00, -9.4606e-01,\n",
      "         1.0514e+00, -4.7390e-01, -5.4566e-01,  9.5592e-01, -5.6309e-01,\n",
      "        -1.0509e+00, -1.4269e+00,  1.8006e-02,  3.5701e-01, -3.5293e-01,\n",
      "        -1.9921e+00, -7.5736e-01, -2.5073e-01,  9.7652e-01, -1.4910e+00,\n",
      "         9.1831e-01,  1.2700e+00, -3.4473e-02,  1.4595e+00, -8.5108e-01,\n",
      "        -1.1615e+00, -5.5348e-01, -3.0333e-01, -9.4580e-01, -1.1448e+00,\n",
      "         3.0422e+00,  6.1324e-01,  3.0350e+00, -6.8923e-01,  9.5389e-02,\n",
      "        -6.7367e-01,  5.2526e-01, -1.3152e-01,  1.8093e-01, -2.9181e-01,\n",
      "         1.7160e+00, -6.6332e-02,  1.3361e-02,  1.3088e+00, -1.9182e+00,\n",
      "        -4.6458e-01,  2.1132e+00,  6.8670e-01, -6.3334e-01,  8.9191e-01,\n",
      "        -2.0521e+00, -1.0686e+00, -1.4730e+00,  2.8534e+00,  2.6000e-01,\n",
      "         1.3450e+00,  1.4973e+00, -3.0963e-02, -1.2930e+00,  5.7220e-02,\n",
      "         1.3452e+00,  1.5518e-01, -1.0208e+00, -2.0433e+00,  5.8151e-02,\n",
      "        -1.2274e+00,  1.0453e+00, -2.9176e-01, -7.2069e-01, -2.6653e-01,\n",
      "         6.5414e-01,  2.5269e+00, -4.1302e-01, -3.5277e-01,  8.6171e-01,\n",
      "        -8.1296e-01, -2.9455e-01,  4.1399e-01, -7.8655e-01,  5.0062e-01,\n",
      "        -2.0316e-01, -2.4171e+00, -1.7120e-01, -4.6577e-01,  1.1467e+00,\n",
      "         1.4463e+00, -2.0860e-01,  3.3092e-01,  5.3465e-01,  1.0071e+00,\n",
      "        -4.9583e-01,  7.5118e-02,  1.1809e+00,  1.8737e-01,  2.2700e-01,\n",
      "         8.1043e-01, -8.4680e-01, -4.5300e-01, -2.0413e+00, -3.8986e-01,\n",
      "        -6.8560e-01,  9.8284e-01,  4.9121e-01,  1.5547e+00, -1.7420e+00,\n",
      "        -5.3723e-02, -1.1367e+00,  3.4782e-01, -1.3668e+00, -5.2284e-01,\n",
      "         2.4871e+00,  3.5001e-01,  1.1956e+00, -9.3289e-02,  1.2136e-01,\n",
      "        -4.8915e-01, -4.4998e-01, -5.7643e-01,  9.7447e-01,  5.7855e-01,\n",
      "        -4.7499e-01, -8.5492e-01,  5.1798e-01,  5.6236e-01,  1.6810e+00,\n",
      "        -2.6849e-01,  1.8298e+00, -4.7705e-01,  6.4414e-01, -6.9470e-01,\n",
      "         1.5108e+00, -1.9497e-01, -1.4577e+00,  4.9817e-01, -1.0480e-02,\n",
      "        -3.0291e-01,  7.7250e-01,  8.3728e-01, -5.6963e-01, -3.0636e-01,\n",
      "        -1.2015e+00, -1.6808e-01, -1.1160e+00, -4.3493e-01, -4.4521e-01,\n",
      "        -2.9466e-02, -3.8111e-01, -4.7001e-01, -4.9397e-01, -1.3041e+00,\n",
      "        -2.1311e-03,  1.8517e+00,  9.8261e-02, -1.7944e-01,  2.8292e-01,\n",
      "         1.2559e+00, -1.0536e+00, -8.5617e-01, -9.4701e-01, -8.0028e-01,\n",
      "         1.8650e+00,  6.9473e-01,  1.1625e+00, -1.3152e+00,  1.3994e-01,\n",
      "         4.6784e-01,  1.4220e-01, -1.3938e+00, -4.0059e-01,  1.6434e+00,\n",
      "         1.2965e+00,  2.0496e-01, -3.2177e-01,  6.0517e-01, -7.0820e-02,\n",
      "         1.2535e+00,  5.1726e-01,  2.3826e+00, -1.1409e+00, -1.3977e+00,\n",
      "        -8.5049e-01, -3.4736e-01,  3.8046e-01,  4.0561e-01,  8.1523e-01,\n",
      "        -3.3189e-01,  4.0425e-01,  3.2908e-01], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Scores:  tensor([ 9.6000e+01,  2.8022e+01,  2.8777e+01,  3.3095e+01,  2.6573e+01,\n",
      "         3.0437e+01,  2.5157e+01,  2.9530e+01,  3.4054e+01,  2.6802e+01,\n",
      "         3.3833e+01,  3.3834e+01, -1.8730e+00, -2.2752e+00, -9.9029e-01,\n",
      "        -7.3643e-01,  1.4924e+00, -2.4197e+00, -5.6498e-01,  1.4917e+00,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "        -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "       grad_fn=<SelectBackward0>) \n",
      "\n",
      "Attention M:  tensor([1.0000e+00, 3.0051e-30, 6.3885e-30, 4.7939e-28, 7.0519e-31, 3.3604e-29,\n",
      "        1.7115e-31, 1.3574e-29, 1.2515e-27, 8.8728e-31, 1.0030e-27, 1.0044e-27,\n",
      "        3.1249e-43, 2.0879e-43, 7.5530e-43, 9.7250e-43, 9.0370e-42, 1.8077e-43,\n",
      "        1.1547e-42, 9.0300e-42, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "emb = Embedding()\n",
    "embeds = emb(input_ids, segment_ids)\n",
    "\n",
    "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
    "print('attenM: ', attenM.size())\n",
    "\n",
    "SDPA= ScaledDotProductAttention()(embeds, embeds, embeds, attenM)\n",
    "\n",
    "S, C, A = SDPA\n",
    "print('S: ', S.size())\n",
    "print('C: ', C.size())\n",
    "print('A: ', A.size())\n",
    "\n",
    "print('Context: ',C[0][0])\n",
    "print()\n",
    "print('Scores: ', S[0][0],'\\n\\nAttention M: ', A[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "hUX_eM_E1B8p"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        print('[MultiHeadAttention]Q: ', Q.size())\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores, context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zs_xOAZy3pay",
    "outputId": "e77fa002-72b7-4904-e0b6-d154af0d3d3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attenM:  torch.Size([6, 30, 30])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0590, 0.0488, 0.0568, 0.1013, 0.0266, 0.0504, 0.0292, 0.0445, 0.0365,\n",
       "         0.0477, 0.0373, 0.0655, 0.0405, 0.0632, 0.0365, 0.0392, 0.0634, 0.0529,\n",
       "         0.0475, 0.0533, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0647, 0.0665, 0.0475, 0.0573, 0.0484, 0.0369, 0.0404, 0.0340, 0.0518,\n",
       "         0.0442, 0.0686, 0.0572, 0.0516, 0.0561, 0.0461, 0.0440, 0.0587, 0.0347,\n",
       "         0.0578, 0.0336, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0473, 0.0414, 0.0454, 0.0620, 0.0253, 0.0781, 0.0379, 0.0552, 0.0494,\n",
       "         0.0686, 0.0422, 0.0412, 0.0417, 0.0622, 0.0555, 0.0473, 0.0489, 0.0333,\n",
       "         0.0476, 0.0693, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0384, 0.0545, 0.0610, 0.0428, 0.0376, 0.0449, 0.0337, 0.0381, 0.0519,\n",
       "         0.0812, 0.0462, 0.0440, 0.0524, 0.0476, 0.0420, 0.0446, 0.0508, 0.0437,\n",
       "         0.0850, 0.0595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0465, 0.0355, 0.0342, 0.0423, 0.0298, 0.0294, 0.0353, 0.0498, 0.0368,\n",
       "         0.0285, 0.0199, 0.0436, 0.0694, 0.0621, 0.0760, 0.0494, 0.0949, 0.0449,\n",
       "         0.0767, 0.0951, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0424, 0.0571, 0.0897, 0.0659, 0.0362, 0.0383, 0.0418, 0.0425, 0.0588,\n",
       "         0.0540, 0.0368, 0.0398, 0.0429, 0.0457, 0.0432, 0.0314, 0.0840, 0.0445,\n",
       "         0.0614, 0.0438, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0577, 0.0554, 0.0372, 0.0515, 0.0267, 0.0425, 0.0362, 0.0572, 0.0445,\n",
       "         0.0443, 0.0226, 0.0449, 0.0526, 0.0632, 0.0748, 0.0578, 0.0805, 0.0350,\n",
       "         0.0495, 0.0658, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0863, 0.0377, 0.0322, 0.0678, 0.0286, 0.0585, 0.0362, 0.0234, 0.0348,\n",
       "         0.0473, 0.0434, 0.0501, 0.0455, 0.0910, 0.0488, 0.0595, 0.0578, 0.0428,\n",
       "         0.0524, 0.0559, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0596, 0.0449, 0.0721, 0.0581, 0.0448, 0.0455, 0.0535, 0.0338, 0.0320,\n",
       "         0.0409, 0.0536, 0.0430, 0.0465, 0.0631, 0.0417, 0.0393, 0.0587, 0.0627,\n",
       "         0.0507, 0.0556, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0585, 0.0404, 0.0319, 0.0329, 0.0308, 0.0349, 0.0371, 0.0616, 0.0491,\n",
       "         0.0255, 0.0389, 0.0439, 0.0614, 0.0402, 0.0898, 0.0565, 0.0617, 0.0620,\n",
       "         0.0588, 0.0841, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0382, 0.0280, 0.0354, 0.0536, 0.0423, 0.0335, 0.0385, 0.0280, 0.0388,\n",
       "         0.0450, 0.0201, 0.0453, 0.0847, 0.0846, 0.0466, 0.0697, 0.0552, 0.0564,\n",
       "         0.0790, 0.0769, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0495, 0.0620, 0.0700, 0.0523, 0.0293, 0.0513, 0.0344, 0.0438, 0.0356,\n",
       "         0.0371, 0.0338, 0.0476, 0.0578, 0.0632, 0.0310, 0.0419, 0.0709, 0.0611,\n",
       "         0.0717, 0.0554, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0365, 0.0537, 0.0910, 0.0446, 0.0593, 0.0364, 0.0474, 0.0264, 0.0410,\n",
       "         0.0453, 0.0576, 0.0793, 0.0248, 0.0635, 0.0472, 0.0257, 0.0444, 0.0508,\n",
       "         0.0407, 0.0844, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0397, 0.0416, 0.0579, 0.0337, 0.0651, 0.0589, 0.0591, 0.0372, 0.0500,\n",
       "         0.0774, 0.0730, 0.0502, 0.0457, 0.0458, 0.0280, 0.0860, 0.0304, 0.0305,\n",
       "         0.0385, 0.0512, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0393, 0.0324, 0.0422, 0.0401, 0.0669, 0.0422, 0.0627, 0.0511, 0.0484,\n",
       "         0.0412, 0.0509, 0.0524, 0.0392, 0.0548, 0.0757, 0.0561, 0.0569, 0.0342,\n",
       "         0.0362, 0.0771, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0465, 0.0489, 0.0471, 0.0425, 0.0299, 0.0487, 0.0382, 0.0422, 0.0497,\n",
       "         0.0298, 0.0590, 0.0536, 0.0610, 0.0378, 0.0522, 0.0727, 0.0532, 0.0370,\n",
       "         0.0657, 0.0843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0459, 0.0438, 0.0334, 0.0508, 0.0537, 0.0503, 0.0581, 0.0546, 0.0553,\n",
       "         0.0410, 0.0671, 0.0353, 0.0484, 0.0523, 0.0723, 0.0636, 0.0403, 0.0422,\n",
       "         0.0356, 0.0560, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0531, 0.0406, 0.0505, 0.0346, 0.0658, 0.0539, 0.0500, 0.0428, 0.0310,\n",
       "         0.0334, 0.0638, 0.0481, 0.0466, 0.0449, 0.0626, 0.0486, 0.0480, 0.0720,\n",
       "         0.0434, 0.0663, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0241, 0.0258, 0.0320, 0.0409, 0.0319, 0.0547, 0.0391, 0.0462, 0.0452,\n",
       "         0.0365, 0.0375, 0.0321, 0.0723, 0.0509, 0.0791, 0.0897, 0.0459, 0.0390,\n",
       "         0.0585, 0.1182, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0463, 0.0400, 0.0445, 0.0546, 0.0577, 0.0494, 0.0540, 0.0511, 0.0370,\n",
       "         0.0475, 0.0579, 0.0502, 0.0467, 0.0538, 0.0441, 0.0579, 0.0543, 0.0340,\n",
       "         0.0536, 0.0653, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0633, 0.0461, 0.0299, 0.0461, 0.0349, 0.0372, 0.0441, 0.0437, 0.0323,\n",
       "         0.0466, 0.0338, 0.0468, 0.0500, 0.0505, 0.0635, 0.0547, 0.0580, 0.0756,\n",
       "         0.0749, 0.0681, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0591, 0.0412, 0.0311, 0.0473, 0.0387, 0.0300, 0.0456, 0.0378, 0.0411,\n",
       "         0.0494, 0.0369, 0.0422, 0.0410, 0.0512, 0.0589, 0.0555, 0.0712, 0.0871,\n",
       "         0.0782, 0.0565, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0661, 0.0510, 0.0269, 0.0454, 0.0216, 0.0306, 0.0225, 0.0372, 0.0390,\n",
       "         0.0363, 0.0202, 0.0391, 0.0450, 0.0564, 0.0457, 0.0814, 0.0855, 0.0683,\n",
       "         0.1000, 0.0816, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0742, 0.0394, 0.0289, 0.0589, 0.0234, 0.0428, 0.0300, 0.0296, 0.0484,\n",
       "         0.0343, 0.0533, 0.0384, 0.0462, 0.0543, 0.0522, 0.0620, 0.0672, 0.0769,\n",
       "         0.0792, 0.0604, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0466, 0.0438, 0.0322, 0.0647, 0.0254, 0.0499, 0.0320, 0.0517, 0.0392,\n",
       "         0.0494, 0.0295, 0.0344, 0.0700, 0.0512, 0.0412, 0.0808, 0.0588, 0.0702,\n",
       "         0.0799, 0.0492, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0493, 0.0598, 0.0231, 0.0574, 0.0342, 0.0409, 0.0391, 0.0533, 0.0394,\n",
       "         0.0530, 0.0404, 0.0530, 0.0475, 0.0444, 0.0546, 0.0574, 0.0523, 0.0878,\n",
       "         0.0477, 0.0653, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0553, 0.0748, 0.0370, 0.0434, 0.0389, 0.0355, 0.0486, 0.0489, 0.0292,\n",
       "         0.0728, 0.0382, 0.0470, 0.0435, 0.0476, 0.0566, 0.0663, 0.0462, 0.0690,\n",
       "         0.0539, 0.0474, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0407, 0.0415, 0.0373, 0.0482, 0.0230, 0.0336, 0.0279, 0.0320, 0.0313,\n",
       "         0.0326, 0.0199, 0.0299, 0.0851, 0.0562, 0.0659, 0.0806, 0.0700, 0.0702,\n",
       "         0.1000, 0.0743, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0458, 0.0521, 0.0235, 0.0689, 0.0282, 0.0428, 0.0332, 0.0540, 0.0374,\n",
       "         0.0425, 0.0346, 0.0363, 0.0388, 0.0395, 0.0597, 0.0778, 0.0530, 0.0748,\n",
       "         0.0763, 0.0808, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0457, 0.0478, 0.0261, 0.0544, 0.0227, 0.0362, 0.0297, 0.0439, 0.0328,\n",
       "         0.0334, 0.0234, 0.0365, 0.0548, 0.0507, 0.0597, 0.0624, 0.0770, 0.0862,\n",
       "         0.0904, 0.0859, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = Embedding()\n",
    "embeds = emb(input_ids, segment_ids)\n",
    "\n",
    "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
    "print('attenM: ', attenM.size())\n",
    "\n",
    "Output, A = MultiHeadAttention()(embeds, embeds, embeds, attenM)\n",
    "\n",
    "\n",
    "A[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "_GQFL_Va4N4Y"
   },
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "RgmfjTqw4Qnw"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "OZ0TJ84W4SZw"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UAG3SEP4UbU",
    "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 5, 9, 6, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14]\n",
      "1 masked_tokens:  [5, 8]\n",
      "1 masked_pos:  [12, 4]\n",
      "2 masked_tokens:  [5, 8, 0, 0, 0]\n",
      "2 masked_pos:  [12, 4, 0, 0, 0]\n",
      "inputs_ids: [1, 19, 11, 4, 12, 28, 25, 16, 2, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "1 masked_tokens:  [11, 8, 11]\n",
      "1 masked_pos:  [2, 12, 11]\n",
      "2 masked_tokens:  [11, 8, 11, 0, 0]\n",
      "2 masked_pos:  [2, 12, 11, 0, 0]\n",
      "inputs_ids: [1, 13, 18, 9, 17, 20, 23, 9, 24, 2, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "1 masked_tokens:  [17, 26, 24]\n",
      "1 masked_pos:  [4, 15, 8]\n",
      "2 masked_tokens:  [17, 26, 24, 0, 0]\n",
      "2 masked_pos:  [4, 15, 8, 0, 0]\n",
      "inputs_ids: [1, 5, 9, 6, 2, 13, 18, 9, 17, 20, 23, 9, 24, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "1 masked_tokens:  [9, 20]\n",
      "1 masked_pos:  [7, 9]\n",
      "2 masked_tokens:  [9, 20, 0, 0, 0]\n",
      "2 masked_pos:  [7, 9, 0, 0, 0]\n",
      "inputs_ids: [1, 19, 11, 4, 12, 28, 25, 16, 2, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "1 masked_tokens:  [13, 8, 19]\n",
      "1 masked_pos:  [15, 12, 1]\n",
      "2 masked_tokens:  [13, 8, 19, 0, 0]\n",
      "2 masked_pos:  [15, 12, 1, 0, 0]\n",
      "inputs_ids: [1, 13, 18, 9, 17, 20, 23, 9, 24, 2, 5, 9, 6, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12]\n",
      "1 masked_tokens:  [18, 20]\n",
      "1 masked_pos:  [2, 5]\n",
      "2 masked_tokens:  [18, 20, 0, 0, 0]\n",
      "2 masked_pos:  [2, 5, 0, 0, 0]\n",
      "inputs_ids: [1, 5, 9, 6, 2, 7, 21, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 5, 6, 7]\n",
      "1 masked_tokens:  [5]\n",
      "1 masked_pos:  [1]\n",
      "2 masked_tokens:  [5, 0, 0, 0, 0]\n",
      "2 masked_pos:  [1, 0, 0, 0, 0]\n",
      "inputs_ids: [1, 15, 20, 23, 9, 14, 27, 6, 2, 7, 21, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]\n",
      "1 masked_tokens:  [7, 26]\n",
      "1 masked_pos:  [9, 11]\n",
      "2 masked_tokens:  [7, 26, 0, 0, 0]\n",
      "2 masked_pos:  [9, 11, 0, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 13, 18, 9, 17, 20, 23, 9, 24, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "1 masked_tokens:  [22, 11, 23]\n",
      "1 masked_pos:  [8, 3, 17]\n",
      "2 masked_tokens:  [22, 11, 23, 0, 0]\n",
      "2 masked_pos:  [8, 3, 17, 0, 0]\n",
      "inputs_ids: [1, 13, 18, 9, 17, 20, 23, 9, 24, 2, 7, 21, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12]\n",
      "1 masked_tokens:  [24, 23]\n",
      "1 masked_pos:  [8, 6]\n",
      "2 masked_tokens:  [24, 23, 0, 0, 0]\n",
      "2 masked_pos:  [8, 6, 0, 0, 0]\n",
      "inputs_ids: [1, 7, 21, 26, 2, 13, 18, 9, 17, 20, 23, 9, 24, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "1 masked_tokens:  [7, 9]\n",
      "1 masked_pos:  [1, 11]\n",
      "2 masked_tokens:  [7, 9, 0, 0, 0]\n",
      "2 masked_pos:  [1, 11, 0, 0, 0]\n",
      "inputs_ids: [1, 7, 21, 26, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11]\n",
      "1 masked_tokens:  [4, 25]\n",
      "1 masked_pos:  [7, 10]\n",
      "2 masked_tokens:  [4, 25, 0, 0, 0]\n",
      "2 masked_pos:  [7, 10, 0, 0, 0]\n",
      "inputs_ids: [1, 15, 6, 11, 8, 10, 26, 13, 22, 18, 9, 2, 7, 21, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14]\n",
      "1 masked_tokens:  [6, 8]\n",
      "1 masked_pos:  [2, 4]\n",
      "2 masked_tokens:  [6, 8, 0, 0, 0]\n",
      "2 masked_pos:  [2, 4, 0, 0, 0]\n",
      "inputs_ids: [1, 19, 11, 4, 12, 28, 25, 16, 2, 13, 18, 9, 17, 20, 23, 9, 24, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "1 masked_tokens:  [23, 4, 13]\n",
      "1 masked_pos:  [14, 3, 9]\n",
      "2 masked_tokens:  [23, 4, 13, 0, 0]\n",
      "2 masked_pos:  [14, 3, 9, 0, 0]\n",
      "inputs_ids: [1, 13, 18, 9, 17, 20, 23, 9, 24, 2, 15, 20, 23, 9, 14, 27, 6, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16]\n",
      "1 masked_tokens:  [18, 24, 20]\n",
      "1 masked_pos:  [2, 8, 5]\n",
      "2 masked_tokens:  [18, 24, 20, 0, 0]\n",
      "2 masked_pos:  [2, 8, 5, 0, 0]\n",
      "inputs_ids: [1, 7, 21, 26, 2, 5, 9, 6, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 5, 6, 7]\n",
      "1 masked_tokens:  [21]\n",
      "1 masked_pos:  [2]\n",
      "2 masked_tokens:  [21, 0, 0, 0, 0]\n",
      "2 masked_pos:  [2, 0, 0, 0, 0]\n",
      "inputs_ids: [1, 15, 20, 23, 9, 14, 27, 6, 2, 19, 11, 4, 12, 28, 25, 16, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]\n",
      "1 masked_tokens:  [9, 14, 4]\n",
      "1 masked_pos:  [4, 5, 11]\n",
      "2 masked_tokens:  [9, 14, 4, 0, 0]\n",
      "2 masked_pos:  [4, 5, 11, 0, 0]\n",
      "inputs_ids: [1, 19, 11, 4, 12, 28, 25, 16, 2, 7, 21, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]\n",
      "1 masked_tokens:  [21, 7]\n",
      "1 masked_pos:  [10, 9]\n",
      "2 masked_tokens:  [21, 7, 0, 0, 0]\n",
      "2 masked_pos:  [10, 9, 0, 0, 0]\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([6, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([6, 12, 30, 64])\n",
      "Epoch: 0010 cost = 34.701595\n"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD3K8T6B4YJp",
    "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thanks you Romeo\n",
      "tensor([ 1, 15,  6, 11,  1, 10, 26, 13, 22, 18,  9,  2,  3,  9,  6,  2,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "['[CLS]', 'hello', 'romeo', 'my', '[CLS]', 'is', 'juliet', 'nice', 'to', 'meet', 'you', '[SEP]', '[MASK]', 'you', 'romeo', '[SEP]']\n",
      "[MultiHeadAttention]Q:  torch.Size([1, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([1, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([1, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([1, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([1, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([1, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([1, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([1, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([1, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([1, 12, 30, 64])\n",
      "[MultiHeadAttention]Q:  torch.Size([1, 30, 768])\n",
      "[ScaledDotProductAttention]Q:  torch.Size([1, 12, 30, 64])\n",
      "masked tokens list :  [5, 8]\n",
      "predict masked tokens list :  [26, 24, 24, 24, 24]\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print(input_ids[0])\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf97uJJS4grJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bidirectional encoder representation from Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
