{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSz5jzj61nHc"
   },
   "source": [
    "This code is possible because of [Tae-Hwan Jung](https://github.com/graykode). I have just broken down the code and added few things here and here for better understanding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-8kZmr4ItGUj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w6YMNvc8tbA9"
   },
   "outputs": [],
   "source": [
    "text = (\n",
    "        'Hello, how are you? I am Romeo.\\n'\n",
    "        'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "        'Nice meet you too. How are you today?\\n'\n",
    "        'Great. My baseball team won the competition.\\n'\n",
    "        'Oh Congratulations, Juliet\\n'\n",
    "        'Thanks you Romeo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AhX8b1ydtrVf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list:  ['my', 'today', 'team', 'won', 'i', 'to', 'congratulations', 'too', 'how', 'great', 'hello', 'you', 'is', 'baseball', 'oh', 'meet', 'name', 'the', 'romeo', 'nice', 'are', 'am', 'juliet', 'thanks', 'competition']\n",
      "token_list: [[14, 12, 24, 15, 8, 25, 22], [14, 22, 4, 20, 16, 26, 23, 9, 19, 15], [23, 19, 15, 11, 12, 24, 15, 5], [13, 4, 17, 6, 7, 21, 28], [18, 10, 26], [27, 15, 22]]\n"
     ]
    }
   ],
   "source": [
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "print('word_list: ', word_list)\n",
    "\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello how are you i am romeo',\n",
       " 'hello romeo my name is juliet nice to meet you',\n",
       " 'nice meet you too how are you today',\n",
       " 'great my baseball team won the competition',\n",
       " 'oh congratulations juliet',\n",
       " 'thanks you romeo']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ42SFLKtsv_",
    "outputId": "16c28ac8-8349-48ab-f1d3-a9431e658349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 12, 24, 15, 8, 25, 22],\n",
       " [14, 22, 4, 20, 16, 26, 23, 9, 19, 15],\n",
       " [23, 19, 15, 11, 12, 24, 15, 5],\n",
       " [13, 4, 17, 6, 7, 21, 28],\n",
       " [18, 10, 26],\n",
       " [27, 15, 22]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q03SGkfIu_Kd"
   },
   "outputs": [],
   "source": [
    "maxlen = 30 # maximum of length\n",
    "batch_size = 6\n",
    "max_pred = 5  # max tokens of prediction\n",
    "n_layers = 6 # number of Encoder of Encoder Layer\n",
    "n_heads = 12 # number of heads in Multi-Head Attention\n",
    "d_model = 768 # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "TtyOOmRntu8w"
   },
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
    "#         print('tokens_a_index:', tokens_a_index)\n",
    "#         print('tokens_b_index:', tokens_b_index)\n",
    "        \n",
    "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "#         print('tokens_a:', tokens_a)\n",
    "        # 选出随机两个句子\n",
    "\n",
    "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "        print('inputs_ids:', input_ids)\n",
    "        # 按照bert的约定，构造输入数据\n",
    "\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        # 前后两个句子的编码\n",
    "\n",
    "        #MASK LM\n",
    "        n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "        # 因为要随机mask掉15%的单词，计算句子15%的长度\n",
    "        \n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "        # 取出可以mask的索引位置\n",
    "        print('cand_maked_pos: ', cand_maked_pos)\n",
    "        \n",
    "        shuffle(cand_maked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        # masked_pos选出要mask的索引\n",
    "        for pos in cand_maked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.8:  # 80%  # 要mask的token有80%的几率替换为[MASK]标记\n",
    "                input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "            elif random() < 0.5:  # 10%  # 剩下有%10的几率被替换为其他token，另外有10%几率原封不动\n",
    "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "                input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)  # 看来对句子嵌入的padding不用考虑现有的是0或1，直接用0填充！！！\n",
    "        print('1 masked_tokens: ', masked_tokens)\n",
    "        print('1 masked_pos: ', masked_pos)\n",
    "        \n",
    "    #     # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:\n",
    "            n_pad = max_pred - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "        print('2 masked_tokens: ', masked_tokens)\n",
    "        print('2 masked_pos: ', masked_pos)\n",
    "\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext(tokens_b是tokens_a的下一个句子)\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext(tokens_b不是tokens_a的下一个句子)\n",
    "            negative += 1\n",
    "    return batch\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "s1PGksqBNuZM"
   },
   "outputs": [],
   "source": [
    "# 获取mask矩阵（）\n",
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    # 入参seq_q、seq_k就是句子的tokens的batch\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "#     print('before expand: ', pad_attn_mask)\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "lgJwW4OaiXE2"
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "Q7_HC-Y0jC3K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_ids: [1, 13, 4, 17, 6, 7, 21, 28, 2, 14, 12, 24, 15, 8, 25, 22, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]\n",
      "1 masked_tokens:  [6, 15, 4]\n",
      "1 masked_pos:  [4, 12, 2]\n",
      "2 masked_tokens:  [6, 15, 4, 0, 0]\n",
      "2 masked_pos:  [4, 12, 2, 0, 0]\n",
      "inputs_ids: [1, 14, 22, 4, 20, 16, 26, 23, 9, 19, 15, 2, 18, 10, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14]\n",
      "1 masked_tokens:  [15, 4]\n",
      "1 masked_pos:  [10, 3]\n",
      "2 masked_tokens:  [15, 4, 0, 0, 0]\n",
      "2 masked_pos:  [10, 3, 0, 0, 0]\n",
      "inputs_ids: [1, 14, 22, 4, 20, 16, 26, 23, 9, 19, 15, 2, 23, 19, 15, 11, 12, 24, 15, 5, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "1 masked_tokens:  [11, 14, 5]\n",
      "1 masked_pos:  [15, 1, 19]\n",
      "2 masked_tokens:  [11, 14, 5, 0, 0]\n",
      "2 masked_pos:  [15, 1, 19, 0, 0]\n",
      "inputs_ids: [1, 13, 4, 17, 6, 7, 21, 28, 2, 27, 15, 22, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]\n",
      "1 masked_tokens:  [15, 17]\n",
      "1 masked_pos:  [10, 3]\n",
      "2 masked_tokens:  [15, 17, 0, 0, 0]\n",
      "2 masked_pos:  [10, 3, 0, 0, 0]\n",
      "inputs_ids: [1, 27, 15, 22, 2, 14, 12, 24, 15, 8, 25, 22, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11]\n",
      "1 masked_tokens:  [22, 8]\n",
      "1 masked_pos:  [11, 9]\n",
      "2 masked_tokens:  [22, 8, 0, 0, 0]\n",
      "2 masked_pos:  [11, 9, 0, 0, 0]\n",
      "inputs_ids: [1, 14, 12, 24, 15, 8, 25, 22, 2, 14, 22, 4, 20, 16, 26, 23, 9, 19, 15, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "1 masked_tokens:  [4, 14, 24]\n",
      "1 masked_pos:  [11, 1, 3]\n",
      "2 masked_tokens:  [4, 14, 24, 0, 0]\n",
      "2 masked_pos:  [11, 1, 3, 0, 0]\n",
      "inputs_ids: [1, 14, 12, 24, 15, 8, 25, 22, 2, 27, 15, 22, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11]\n",
      "1 masked_tokens:  [25, 22]\n",
      "1 masked_pos:  [6, 7]\n",
      "2 masked_tokens:  [25, 22, 0, 0, 0]\n",
      "2 masked_pos:  [6, 7, 0, 0, 0]\n",
      "inputs_ids: [1, 14, 12, 24, 15, 8, 25, 22, 2, 14, 12, 24, 15, 8, 25, 22, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]\n",
      "1 masked_tokens:  [22, 25, 24]\n",
      "1 masked_pos:  [15, 14, 11]\n",
      "2 masked_tokens:  [22, 25, 24, 0, 0]\n",
      "2 masked_pos:  [15, 14, 11, 0, 0]\n",
      "inputs_ids: [1, 23, 19, 15, 11, 12, 24, 15, 5, 2, 23, 19, 15, 11, 12, 24, 15, 5, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "1 masked_tokens:  [12, 15, 5]\n",
      "1 masked_pos:  [14, 12, 8]\n",
      "2 masked_tokens:  [12, 15, 5, 0, 0]\n",
      "2 masked_pos:  [14, 12, 8, 0, 0]\n",
      "inputs_ids: [1, 14, 22, 4, 20, 16, 26, 23, 9, 19, 15, 2, 14, 22, 4, 20, 16, 26, 23, 9, 19, 15, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "1 masked_tokens:  [14, 16, 23]\n",
      "1 masked_pos:  [12, 5, 7]\n",
      "2 masked_tokens:  [14, 16, 23, 0, 0]\n",
      "2 masked_pos:  [12, 5, 7, 0, 0]\n",
      "inputs_ids: [1, 14, 12, 24, 15, 8, 25, 22, 2, 23, 19, 15, 11, 12, 24, 15, 5, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "1 masked_tokens:  [12, 15, 14]\n",
      "1 masked_pos:  [2, 15, 1]\n",
      "2 masked_tokens:  [12, 15, 14, 0, 0]\n",
      "2 masked_pos:  [2, 15, 1, 0, 0]\n",
      "inputs_ids: [1, 23, 19, 15, 11, 12, 24, 15, 5, 2, 18, 10, 26, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12]\n",
      "1 masked_tokens:  [19, 23]\n",
      "1 masked_pos:  [2, 1]\n",
      "2 masked_tokens:  [19, 23, 0, 0, 0]\n",
      "2 masked_pos:  [2, 1, 0, 0, 0]\n",
      "inputs_ids: [1, 27, 15, 22, 2, 14, 12, 24, 15, 8, 25, 22, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11]\n",
      "1 masked_tokens:  [24, 12]\n",
      "1 masked_pos:  [7, 6]\n",
      "2 masked_tokens:  [24, 12, 0, 0, 0]\n",
      "2 masked_pos:  [7, 6, 0, 0, 0]\n",
      "inputs_ids: [1, 13, 4, 17, 6, 7, 21, 28, 2, 14, 12, 24, 15, 8, 25, 22, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15]\n",
      "1 masked_tokens:  [21, 28, 17]\n",
      "1 masked_pos:  [6, 7, 3]\n",
      "2 masked_tokens:  [21, 28, 17, 0, 0]\n",
      "2 masked_pos:  [6, 7, 3, 0, 0]\n",
      "inputs_ids: [1, 14, 22, 4, 20, 16, 26, 23, 9, 19, 15, 2, 27, 15, 22, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14]\n",
      "1 masked_tokens:  [27, 15]\n",
      "1 masked_pos:  [12, 13]\n",
      "2 masked_tokens:  [27, 15, 0, 0, 0]\n",
      "2 masked_pos:  [12, 13, 0, 0, 0]\n",
      "inputs_ids: [1, 14, 22, 4, 20, 16, 26, 23, 9, 19, 15, 2, 23, 19, 15, 11, 12, 24, 15, 5, 2]\n",
      "cand_maked_pos:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "1 masked_tokens:  [24, 23, 19]\n",
      "1 masked_pos:  [17, 7, 13]\n",
      "2 masked_tokens:  [24, 23, 19, 0, 0]\n",
      "2 masked_pos:  [17, 7, 13, 0, 0]\n"
     ]
    }
   ],
   "source": [
    " batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "XM1-FdPJi6p3"
   },
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "# zip(*batch)将每个batch对应的向量匹配起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 13, 16, 17,  3,  7, 21, 28,  2, 14, 12, 24,  3,  8, 25, 22,  2,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 14, 22, 24, 20, 16, 26, 23,  9, 19, 15,  2, 18, 10, 26,  2,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3, 22,  4, 20, 16, 26, 23,  9, 19, 15,  2, 23, 19, 15,  3, 12, 24,\n",
       "         15,  3,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 13,  4, 17,  6,  7, 21, 28,  2, 27,  3, 22,  2,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1,  3, 12,  3, 15,  8, 25, 22,  2, 14, 22,  3, 20, 16, 26, 23,  9, 19,\n",
       "         15,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 14, 22,  4, 20, 16, 26, 11,  9, 19, 15,  2, 23,  3, 15, 11, 12, 24,\n",
       "         15,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhM1DCU_iYCB",
    "outputId": "7525fdc7-f78e-488b-ef35-7d9ecfb969e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]),\n",
       " tensor([ 1, 13, 16, 17,  3,  7, 21, 28,  2, 14, 12, 24,  3,  8, 25, 22,  2,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attn_pad_mask(input_ids, input_ids)[0][0], input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "Qnay0LTDjE4S"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        print('pos: ', pos)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos:  tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6358, -0.4575,  1.0810,  ...,  1.5557, -1.0078,  0.9531],\n",
       "         [ 0.4713, -1.2767, -1.3843,  ...,  0.3360, -3.0993,  1.0970],\n",
       "         [ 2.0783,  0.8257, -1.3102,  ..., -0.4824, -0.3799,  1.0249],\n",
       "         ...,\n",
       "         [ 1.1249, -0.5074,  0.8849,  ..., -0.7842, -3.2319,  1.1591],\n",
       "         [ 0.3625,  0.8731,  0.2290,  ..., -0.3683, -3.2458,  1.6935],\n",
       "         [ 0.6144, -0.2211, -0.7120,  ..., -0.3630, -1.1834,  0.9578]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testEmbed = Embedding()\n",
    "testEmbed(input_ids[0].unsqueeze(0), segment_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "rHjj-1wXjsdI"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8X2rbGNMzl7o",
    "outputId": "81b17de0-c3ef-448e-896f-7143b39cbd62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos:  tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
      "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g2/jy_8z3fs28526pxbf3l1gbdm0000gn/T/ipykernel_44273/3129892083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mSDPA\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mScaledDotProductAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattenM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSDPA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Masks'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "emb = Embedding()\n",
    "embeds = emb(input_ids, segment_ids)\n",
    "\n",
    "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
    "\n",
    "SDPA= ScaledDotProductAttention()(embeds, embeds, embeds, attenM)\n",
    "\n",
    "S, C, A = SDPA\n",
    "\n",
    "print('Masks',masks[0][0])\n",
    "print()\n",
    "print('Scores: ', S[0][0],'\\n\\nAttention M: ', A[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUX_eM_E1B8p"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zs_xOAZy3pay",
    "outputId": "e77fa002-72b7-4904-e0b6-d154af0d3d3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0571, 0.1222, 0.0787, 0.0652, 0.0746, 0.1160, 0.0649, 0.1006, 0.0865,\n",
       "         0.0523, 0.0549, 0.0845, 0.0424, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0593, 0.0803, 0.0766, 0.0923, 0.0775, 0.1272, 0.0426, 0.1257, 0.1050,\n",
       "         0.0458, 0.0581, 0.0641, 0.0457, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0646, 0.0927, 0.0880, 0.0506, 0.0807, 0.0873, 0.0694, 0.0641, 0.0730,\n",
       "         0.0681, 0.0877, 0.0701, 0.1035, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0484, 0.0963, 0.0904, 0.0637, 0.0775, 0.0989, 0.0654, 0.0769, 0.0737,\n",
       "         0.0740, 0.0996, 0.0537, 0.0815, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0751, 0.1262, 0.0653, 0.0443, 0.0758, 0.1557, 0.0536, 0.0561, 0.1129,\n",
       "         0.0441, 0.1034, 0.0177, 0.0697, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0291, 0.0631, 0.0465, 0.0562, 0.0828, 0.1244, 0.0543, 0.0721, 0.0579,\n",
       "         0.1160, 0.1073, 0.0998, 0.0904, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0438, 0.0666, 0.0548, 0.0532, 0.0773, 0.1461, 0.0823, 0.0597, 0.0608,\n",
       "         0.0692, 0.1033, 0.0894, 0.0935, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0768, 0.0837, 0.0792, 0.0591, 0.0594, 0.0728, 0.0607, 0.0472, 0.0640,\n",
       "         0.0805, 0.0793, 0.1730, 0.0641, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0861, 0.0724, 0.0786, 0.0851, 0.0482, 0.0894, 0.0620, 0.0988, 0.0614,\n",
       "         0.0785, 0.1007, 0.0649, 0.0740, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0657, 0.0966, 0.0707, 0.0772, 0.0788, 0.0864, 0.0671, 0.0674, 0.0802,\n",
       "         0.0790, 0.0825, 0.0815, 0.0668, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0770, 0.0586, 0.0714, 0.1439, 0.0912, 0.0828, 0.0480, 0.1136, 0.0852,\n",
       "         0.0668, 0.0526, 0.0611, 0.0478, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0558, 0.0349, 0.0811, 0.0748, 0.0728, 0.0662, 0.0452, 0.0619, 0.0422,\n",
       "         0.1138, 0.1070, 0.1652, 0.0789, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.1015, 0.0435, 0.0663, 0.0926, 0.0938, 0.0668, 0.0645, 0.0949, 0.0520,\n",
       "         0.0831, 0.0808, 0.0657, 0.0945, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0583, 0.0705, 0.0481, 0.0522, 0.0741, 0.1113, 0.0603, 0.0776, 0.0634,\n",
       "         0.0929, 0.0856, 0.1118, 0.0940, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0712, 0.0786, 0.0434, 0.0592, 0.0590, 0.1091, 0.0567, 0.0839, 0.0577,\n",
       "         0.0729, 0.1156, 0.1091, 0.0836, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0466, 0.0619, 0.0565, 0.0554, 0.0631, 0.1349, 0.0604, 0.0867, 0.0576,\n",
       "         0.0642, 0.0848, 0.1130, 0.1148, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0585, 0.0810, 0.0521, 0.0614, 0.0590, 0.1550, 0.0527, 0.0831, 0.0609,\n",
       "         0.0738, 0.0786, 0.0970, 0.0870, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0922, 0.0782, 0.0623, 0.0544, 0.0671, 0.1160, 0.0476, 0.1040, 0.0573,\n",
       "         0.0933, 0.0804, 0.0721, 0.0751, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0743, 0.0779, 0.0476, 0.0558, 0.0831, 0.1164, 0.0747, 0.0860, 0.0584,\n",
       "         0.0751, 0.0769, 0.0964, 0.0773, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0813, 0.0851, 0.0709, 0.0933, 0.0578, 0.1176, 0.0591, 0.0835, 0.0670,\n",
       "         0.0632, 0.0756, 0.0744, 0.0710, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0501, 0.0816, 0.0433, 0.0551, 0.0676, 0.1378, 0.0727, 0.0696, 0.0441,\n",
       "         0.0810, 0.0665, 0.1462, 0.0845, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0742, 0.1016, 0.0563, 0.0716, 0.0667, 0.1203, 0.0733, 0.0863, 0.0760,\n",
       "         0.0627, 0.0770, 0.0735, 0.0604, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0911, 0.0746, 0.0696, 0.0746, 0.0573, 0.1212, 0.0470, 0.0882, 0.0512,\n",
       "         0.0686, 0.0789, 0.1072, 0.0704, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0814, 0.0605, 0.0654, 0.0587, 0.0889, 0.1299, 0.0720, 0.0724, 0.0627,\n",
       "         0.0814, 0.0597, 0.0978, 0.0691, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0751, 0.0849, 0.0567, 0.0731, 0.0569, 0.1372, 0.0527, 0.0919, 0.0583,\n",
       "         0.0653, 0.0743, 0.1007, 0.0729, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0786, 0.0957, 0.0631, 0.0846, 0.0446, 0.1231, 0.0811, 0.0794, 0.0652,\n",
       "         0.0593, 0.0769, 0.0753, 0.0730, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0489, 0.0528, 0.0699, 0.0598, 0.0644, 0.1259, 0.0595, 0.0657, 0.0500,\n",
       "         0.1106, 0.0692, 0.1293, 0.0941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0746, 0.0929, 0.0654, 0.0521, 0.0435, 0.1265, 0.0427, 0.1291, 0.0773,\n",
       "         0.0613, 0.0590, 0.0945, 0.0811, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0603, 0.0550, 0.0664, 0.0691, 0.0673, 0.1260, 0.0564, 0.1021, 0.0570,\n",
       "         0.0868, 0.0796, 0.0922, 0.0819, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000],\n",
       "        [0.0865, 0.0825, 0.0790, 0.0822, 0.0615, 0.1391, 0.0461, 0.0915, 0.0697,\n",
       "         0.0691, 0.0576, 0.0696, 0.0655, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = Embedding()\n",
    "embeds = emb(input_ids, segment_ids)\n",
    "\n",
    "attenM = get_attn_pad_mask(input_ids, input_ids)\n",
    "\n",
    "MHA= MultiHeadAttention()(embeds, embeds, embeds, attenM)\n",
    "\n",
    "Output, A = MHA\n",
    "\n",
    "A[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GQFL_Va4N4Y"
   },
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgmfjTqw4Qnw"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ0TJ84W4SZw"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UAG3SEP4UbU",
    "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0010 cost = 44.218983\n"
     ]
    }
   ],
   "source": [
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD3K8T6B4YJp",
    "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\n",
      "Hello, Romeo My name is Juliet. Nice to meet you.\n",
      "Nice meet you too. How are you today?\n",
      "Great. My baseball team won the competition.\n",
      "Oh Congratulations, Juliet\n",
      "Thanks you Romeo\n",
      "['[CLS]', 'nice', 'meet', 'you', 'too', 'how', 'are', 'you', 'today', '[SEP]', '[MASK]', 'congratulations', '[MASK]', '[SEP]']\n",
      "masked tokens list :  [27, 22]\n",
      "predict masked tokens list :  []\n",
      "isNext :  False\n",
      "predict isNext :  True\n"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf97uJJS4grJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bidirectional encoder representation from Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
